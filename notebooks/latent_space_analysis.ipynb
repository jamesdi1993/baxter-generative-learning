{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into jupyter\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "def load_csv_file(filename, skiprows = 0):\n",
    "  return np.loadtxt(open(filename, \"rb\"), delimiter=\",\", skiprows = skiprows)\n",
    "\n",
    "def plot_2d_configs(x1, x2):\n",
    "    \"\"\"\n",
    "    Scatter-plot the 2D array;\n",
    "    \"\"\"\n",
    "    data = (x1, x2)\n",
    "    colors = (\"blue\", \"red\")\n",
    "    groups = (\"collision-free\", \"in-collision\") \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    for points, color, group in zip(data, colors, groups):\n",
    "        print(\"The shape of point is: %s\" % (points.shape, ))\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "    plt.title('In-collision vs. collision-free data points')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d_configs(x1, x2):\n",
    "    \"\"\"\n",
    "    Scatter-plot a three dimensional data\n",
    "    \"\"\"\n",
    "\n",
    "    data = (x1, x2)\n",
    "    colors = (\"blue\", \"red\")\n",
    "    groups = (\"collision-free\", \"in-collision\") \n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    for points, color, group in zip(data, colors, groups):\n",
    "        print(\"The shape of point is: %s\" % (points.shape, ))\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        z = points[:, 2]\n",
    "        ax.scatter(x, y, z, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "    plt.title('In-collision vs. collision-free data points')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_3d_single(points, group, color='red'):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_single(points, group, color='red'):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(points[:,0], points[:,1], alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "def test_plot_2d():\n",
    "    N = 60\n",
    "    g1 = np.array((0.6 + 0.6 * np.random.rand(N), np.random.rand(N))).T\n",
    "    g2 = np.array((0.4+0.3 * np.random.rand(N), 0.5*np.random.rand(N))).T\n",
    "    plot_2d_configs(g1, g2)\n",
    "    \n",
    "def test_plot_3d():\n",
    "    N = 60\n",
    "    g1 = np.array((0.6 + 0.6 * np.random.rand(N), np.random.rand(N),0.4+0.1*np.random.rand(N))).T\n",
    "    g2 = np.array((0.4+0.3 * np.random.rand(N), 0.5*np.random.rand(N),0.1*np.random.rand(N))).T\n",
    "    plot_3d_configs(g1, g2)\n",
    "    \n",
    "test_plot_2d()\n",
    "test_plot_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "num_joints = 7\n",
    "limit = 10000000  # set limit on number of points in the configuration\n",
    "path = \"../data/input/right_joint_states\"\n",
    "file_name = ''\n",
    "\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "for f in files:\n",
    "    \n",
    "    joints = int(f.split('_')[1])\n",
    "    num_samples = int(f.split('_')[2])\n",
    "    if joints == num_joints and num_samples < limit:\n",
    "        file_name = join(path, f)\n",
    "        break\n",
    "print(\"Loading data from file: %s\" % (file_name,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Load and preprocess data; \n",
    "\"\"\"\n",
    "\n",
    "def normalize_data(X, limits):\n",
    "    \"\"\"\n",
    "    Normalize the revolute joint data from joints limits to [0, 1]\n",
    "    X: Data before normalization n x d array\n",
    "    limits: joint limits for the revolute joints, a 2 x d array\n",
    "    \"\"\"\n",
    "    X = X - limits[0, :]\n",
    "    x_range = limits[1, :] - limits[0, :]\n",
    "    return X / x_range\n",
    "\n",
    "def recover_data(normalized_X, limits):\n",
    "    \"\"\"\n",
    "    Recover the normalized X based on joint limits.\n",
    "    normalized_X: the normalized data points;\n",
    "    limits: the joint limits\n",
    "    \"\"\"\n",
    "    x_range = limits[1, :] - limits[0, :]\n",
    "    X = normalized_X * x_range\n",
    "    return X + limits[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINT_NAMES = ['s0','s1','e0','e1','w0','w1','w2']\n",
    "JOINT_LIMITS = {\n",
    "    's0': [-1.7016, 1.7016],\n",
    "    's1': [-2.147, 1.047],\n",
    "    'e0': [-3.0541, 3.0541],\n",
    "    'e1': [-0.05, 2.618],\n",
    "    'w0': [-3.059, 3.059],\n",
    "    'w1': [-1.5707, 2.094],\n",
    "    'w2': [-3.059, 3.059]\n",
    "}\n",
    "collisionKey = 'collisionFree' \n",
    "HEADERS = ['right_' + joint_name for joint_name in JOINT_NAMES]\n",
    "\n",
    "\n",
    "selected_joints = JOINT_NAMES[0:num_joints]\n",
    "joint_limits = [JOINT_LIMITS[joint] for joint in selected_joints]\n",
    "joint_limits = np.array(joint_limits).T\n",
    "headers = ['right_' + joint_name for joint_name in selected_joints]\n",
    "\n",
    "headers_w_collision = headers.copy()\n",
    "headers_w_collision.append('collisionFree')\n",
    "\n",
    "print(\"The joint limits are: %s\" % joint_limits)\n",
    "print(\"The headers are: %s\" % headers)\n",
    "print(\"The headers with collisions are: %s\" % headers_w_collision)\n",
    "print(\"The complete headers are: %s\" % HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "batch_size = 1000\n",
    "learning_rate = 0.0001\n",
    "d_input = num_joints\n",
    "h_dim1 = 256\n",
    "h_dim2 = 100\n",
    "generated_sample_size = 1000000\n",
    "epochs = 10\n",
    "\n",
    "# Latent_variables\n",
    "d_output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(file_name, usecols=headers_w_collision)\n",
    "\n",
    "# Additional step to preserve the header order when reading data; \n",
    "data = input_data[headers_w_collision].values\n",
    "\n",
    "# np.random.shuffle(data)\n",
    "print(\"The first 5 values from the dataset are: %s\" % data[0:5, :])\n",
    "X = normalize_data(data[:, :-1], joint_limits)\n",
    "y = data[:, -1]\n",
    "\n",
    "print(\"The first 5 values after normalization are: %s\" % X[0:5, :])\n",
    "\n",
    "print(\"The max of X is: %s; The min of X is: %s\" % (np.max(X), np.min(X),))\n",
    "\n",
    "cutoff = math.ceil(X.shape[0] * 0.8) # 80 percent training data\n",
    "train_X = X[:cutoff, :]\n",
    "test_X = X[cutoff:, :]\n",
    "train_y = y[:cutoff]\n",
    "test_y = y[cutoff:]\n",
    "\n",
    "train_data = np.hstack((train_X, np.expand_dims(train_y, axis = 1)))\n",
    "test_data = np.hstack((test_X, np.expand_dims(test_y, axis = 1)))\n",
    "\n",
    "print(\"The shape of samples in training is: %s\" % (train_data.shape,))\n",
    "print(\"The shape of samples in test is: %s\" % (test_data.shape,))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 1000 random points to display\n",
    "indices = np.random.choice(train_X.shape[0], 2000, replace=False)\n",
    "train_X_random = train_X[indices, :]\n",
    "train_y_random = train_y[indices]\n",
    "\n",
    "x_free = train_X_random[train_y_random == 1]\n",
    "x_col = train_X_random[train_y_random == 0]\n",
    "\n",
    "plot_2d_configs(x_free[:, 0:2], x_col[:, 0:2]) # s0, s1\n",
    "plot_2d_configs(x_free[:, 1:3], x_col[:, 1:3]) # s1, e0\n",
    "plot_2d_configs(x_free[:, (0,2)], x_col[:, (0,2)]) #s0, e0\n",
    "plot_3d_configs(x_free[:, :3], x_col[:, :3]) # s0, s1, e0\n",
    "plot_3d_single(x_free[:, :3], 'collision-free', 'blue')\n",
    "plot_3d_single(x_free[:, :3], 'in-collision', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "  def __init__(self, d_in, h_dim1, h_dim2, d_out, keep_prob=0):\n",
    "    super(VAE, self).__init__()\n",
    "    # The latent variable is defined as z = N(\\mu, \\sigma).\n",
    "\n",
    "    # Encoder network\n",
    "    self.fc1 = torch.nn.Linear(d_in, h_dim1)\n",
    "    self.fc2 = torch.nn.Linear(h_dim1, h_dim2)\n",
    "    self.mean = torch.nn.Linear(h_dim2, d_out)\n",
    "    self.logvar = torch.nn.Linear(h_dim2, d_out)\n",
    "\n",
    "    # Decoder network\n",
    "    self.fc3 = torch.nn.Linear(d_out, h_dim2)\n",
    "    self.fc4 = torch.nn.Linear(h_dim2, h_dim1)\n",
    "    self.fc5 = torch.nn.Linear(h_dim1, d_in)\n",
    "\n",
    "  # Encode the input into a normal distribution\n",
    "  def encode(self, x):\n",
    "    h1 = F.elu(self.fc1(x))\n",
    "    h2 = F.elu(self.fc2(h1))\n",
    "    return self.mean(h2), self.logvar(h2)\n",
    "\n",
    "  # Reparametrize the normal;\n",
    "  def reparameterize(self, mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "  def decode(self, z):\n",
    "    h2 = F.elu(self.fc3(z))\n",
    "    h1 = F.elu(self.fc4(h2))\n",
    "    return torch.sigmoid(self.fc5(h1))\n",
    "\n",
    "  # Forward pass;\n",
    "  def forward(self, x):\n",
    "    # Change the array to float first;\n",
    "    mu, logvar = self.encode(x.view(-1, num_joints))\n",
    "    # print(\"The value of mu and logvars are: %s, %s\" % (mu, logvar))\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, num_joints), reduction='sum')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def train(model, optimizer, device, epoch, train_loader):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  bce_loss = 0\n",
    "  kld_loss = 0\n",
    "  for batch_idx, batch in enumerate(train_loader):\n",
    "    configs = batch[:, :-1]\n",
    "    y = batch[:, -1:]\n",
    "    configs = configs.to(device).float()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    recon_batch, mu, logvar = model(configs)\n",
    "    # print(\"Max in the reconstructed_batch: %s; Max in the original dataset: %s\" % (np.max(recon_batch.detach().numpy()), np.max(configs.detach().numpy()),))\n",
    "    # print(\"Min in the reconstructed_batch: %s; Min in the original dataset: %s\" % (np.min(recon_batch.detach().numpy()), np.min(configs.detach().numpy()),))\n",
    "    loss, bce, kld = loss_function(recon_batch, configs, mu, logvar)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    train_loss += loss.item()\n",
    "    bce_loss += bce.item()\n",
    "    kld_loss += kld.item()\n",
    "    optimizer.step()\n",
    "  print('====> Epoch: {} Average total training_Loss: {:.4f}; BCE loss: {:.4f}; KLD: {:.4f}'.format(\n",
    "    epoch, train_loss / len(train_loader.dataset), \n",
    "    bce_loss / len(train_loader.dataset), \n",
    "    kld_loss / len(train_loader.dataset)))\n",
    "\n",
    "  # return the last batch\n",
    "  mu = np.hstack((mu.detach().numpy(), y))\n",
    "  return mu, logvar\n",
    "def test(model, epoch, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for i, batch in enumerate(test_loader):\n",
    "        configs = batch[:, :-1]\n",
    "        y = batch[:, -1:]\n",
    "        configs = configs.to(device).float()\n",
    "        recon_batch, mu, logvar = model(configs)\n",
    "        loss, bce, kld = loss_function(recon_batch, configs, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Test_loss: {:.4f};'.format(epoch, test_loss))\n",
    "    mu = np.hstack((mu.detach().numpy(), y))\n",
    "    return test_loss, mu, logvar\n",
    "\n",
    "def generate_samples(generated_batch_size, d_output, device, model):\n",
    "  # Generate sampled outputs;\n",
    "  with torch.no_grad():\n",
    "    norms = torch.randn(generated_batch_size, d_output).to(device)\n",
    "    samples = model.decode(norms).cpu().numpy()\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(d_input, h_dim1, h_dim2, d_output).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model_path = \"./model\"\n",
    "\n",
    "output_directory = model_path + '/' + str(num_joints) + '-' + str(h_dim1) + '-' + str(h_dim2) + '-' + str(d_output) + '-' + str(batch_size) + '-' + \\\n",
    "          str(learning_rate)\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)\n",
    "\n",
    "test_loss = math.inf\n",
    "for epoch in range(1, epochs + 1):\n",
    "  mu, logvar = train(model=model, optimizer=optimizer, device=device, epoch=epoch, train_loader=train_loader)\n",
    "  # print(\"The shape of mu is: %s\" % (mu.shape, ))\n",
    "  test_loss, mu_test, logvar_test = test(model=model, epoch=epoch, device=device, test_loader=test_loader)\n",
    "\n",
    "# Print out metric for evaluation.\n",
    "print(\"Final average test loss: {:.4f};\".format(test_loss))\n",
    "\n",
    "mu_free = mu_test[mu_test[:, -1] == 1]\n",
    "mu_collision = mu_test[mu_test[:, -1] == 0]\n",
    "plot_2d_configs(mu_free[:, :2], mu_collision[:, :2])\n",
    "plot_2d_single(mu_free[:, :2], 'collision-free', 'blue')\n",
    "plot_2d_single(mu_collision[:, :2], 'in-collision', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
