{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into jupyter\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "def load_csv_file(filename, skiprows = 0):\n",
    "  return np.loadtxt(open(filename, \"rb\"), delimiter=\",\", skiprows = skiprows)\n",
    "\n",
    "def plot_configuration(config):\n",
    "  \"\"\"\n",
    "  Scatter-plot the 2D array and return the figure;\n",
    "  :param config: a n x 2 tensor/ndarray \n",
    "  \"\"\"\n",
    "  figure = plt.figure(figsize = (5,5))\n",
    "  plt.scatter(config[:,0], config[:,1])\n",
    "  plt.show()\n",
    "  return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: ../data/input/right_joint_states/right_7_3556224_1548650769.0.csv\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "num_joints = 7\n",
    "limit = 10000000  # set limit on number of points in the configuration\n",
    "path = \"../data/input/right_joint_states\"\n",
    "file_name = ''\n",
    "\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "for f in files:\n",
    "    \n",
    "    joints = int(f.split('_')[1])\n",
    "    num_samples = int(f.split('_')[2])\n",
    "    if joints == num_joints and num_samples < limit:\n",
    "        file_name = join(path, f)\n",
    "        break\n",
    "print(\"Loading data from file: %s\" % (file_name,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Load and preprocess data; \n",
    "\"\"\"\n",
    "\n",
    "def normalize_data(X, limits):\n",
    "    \"\"\"\n",
    "    Normalize the revolute joint data from joints limits to [0, 1]\n",
    "    X: Data before normalization n x d array\n",
    "    limits: joint limits for the revolute joints, a 2 x d array\n",
    "    \"\"\"\n",
    "    X = X - limits[0, :]\n",
    "    x_range = limits[1, :] - limits[0, :]\n",
    "    return X / x_range\n",
    "\n",
    "def recover_data(normalized_X, limits):\n",
    "    \"\"\"\n",
    "    Recover the normalized X based on joint limits.\n",
    "    normalized_X: the normalized data points;\n",
    "    limits: the joint limits\n",
    "    \"\"\"\n",
    "    x_range = limits[1, :] - limits[0, :]\n",
    "    X = normalized_X * x_range\n",
    "    return X + limits[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.52570475 0.82739848 0.41708185]]\n",
      "[[-1.7016 -2.147  -3.0541]\n",
      " [ 1.7016  1.047   3.0541]\n",
      " [ 0.     -0.55    0.    ]]\n"
     ]
    }
   ],
   "source": [
    "def test_normalize_data():\n",
    "    X = np.array([\n",
    "        [-1.7016, -2.147, -3.0541],\n",
    "        [1.7016, 1.047, 3.0541],\n",
    "        [0.087478421724874522,0.49571075835981837, -0.50648066618520682]\n",
    "    ])\n",
    "    limits = np.array([\n",
    "        [-1.7016,-2.147,-3.0541],\n",
    "        [1.7016, 1.047, 3.0541]\n",
    "    ])\n",
    "    print(normalize_data(X, limits))    \n",
    "    \n",
    "def test_recover_data():\n",
    "    X = np.array([\n",
    "        [0, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [0.5,0.5, 0.5]\n",
    "    ])\n",
    "    limits = np.array([\n",
    "        [-1.7016,-2.147,-3.0541],\n",
    "        [1.7016, 1.047, 3.0541]\n",
    "    ])\n",
    "    print(recover_data(X, limits))\n",
    "\n",
    "test_normalize_data()\n",
    "test_recover_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joint limits are: [[-1.7016 -2.147  -3.0541 -0.05   -3.059  -1.5707 -3.059 ]\n",
      " [ 1.7016  1.047   3.0541  2.618   3.059   2.094   3.059 ]]\n",
      "The headers are: ['right_s0', 'right_s1', 'right_e0', 'right_e1', 'right_w0', 'right_w1', 'right_w2']\n",
      "The headers with collisions are: ['right_s0', 'right_s1', 'right_e0', 'right_e1', 'right_w0', 'right_w1', 'right_w2', 'inCollision']\n",
      "The complete headers are: ['right_s0', 'right_s1', 'right_e0', 'right_e1', 'right_w0', 'right_w1', 'right_w2']\n"
     ]
    }
   ],
   "source": [
    "JOINT_NAMES = ['s0','s1','e0','e1','w0','w1','w2']\n",
    "JOINT_LIMITS = {\n",
    "    's0': [-1.7016, 1.7016],\n",
    "    's1': [-2.147, 1.047],\n",
    "    'e0': [-3.0541, 3.0541],\n",
    "    'e1': [-0.05, 2.618],\n",
    "    'w0': [-3.059, 3.059],\n",
    "    'w1': [-1.5707, 2.094],\n",
    "    'w2': [-3.059, 3.059]\n",
    "}\n",
    "\n",
    "collisionKey = 'inCollision' \n",
    "HEADERS = ['right_' + joint_name for joint_name in JOINT_NAMES]\n",
    "\n",
    "selected_joints = JOINT_NAMES[0:num_joints]\n",
    "joint_limits = [JOINT_LIMITS[joint] for joint in selected_joints]\n",
    "joint_limits = np.array(joint_limits).T\n",
    "headers = ['right_' + joint_name for joint_name in selected_joints]\n",
    "\n",
    "headers_w_collision = headers.copy()\n",
    "headers_w_collision.append('inCollision')\n",
    "\n",
    "print(\"The joint limits are: %s\" % joint_limits)\n",
    "print(\"The headers are: %s\" % headers)\n",
    "print(\"The headers with collisions are: %s\" % headers_w_collision)\n",
    "print(\"The complete headers are: %s\" % HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "d_input = num_joints\n",
    "h_dim1 = 256\n",
    "h_dim2 = 100\n",
    "kld_weight = 0.5\n",
    "generated_sample_size = 1000000\n",
    "epochs = 10\n",
    "# Latent_variables\n",
    "d_output = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 values from the dataset are: [[-0.79940635  0.84020634 -1.17697564  1.75967687 -0.68995538  1.10311413\n",
      "   2.14137371  1.        ]\n",
      " [ 0.99436986 -1.1886719  -0.69034255  1.34111893  1.02919112  0.72015444\n",
      "  -0.38291944  1.        ]\n",
      " [ 0.85896437 -0.62246461  0.36769169  0.33578202 -0.15364734  1.88722364\n",
      "  -1.33448359  1.        ]\n",
      " [ 0.43601655 -0.02050567  1.74412857  1.97655695  2.93276584 -0.19083805\n",
      "  -2.34960527  1.        ]\n",
      " [ 1.30076617 -0.49289148  2.39787682  1.18388345 -0.67348273 -0.2425781\n",
      "  -0.00924419  0.        ]]\n",
      "The first 5 values after normalization are: [[-0.79940635  0.84020634 -1.17697564  1.75967687 -0.68995538  1.10311413\n",
      "   2.14137371]\n",
      " [ 0.99436986 -1.1886719  -0.69034255  1.34111893  1.02919112  0.72015444\n",
      "  -0.38291944]\n",
      " [ 0.85896437 -0.62246461  0.36769169  0.33578202 -0.15364734  1.88722364\n",
      "  -1.33448359]\n",
      " [ 0.43601655 -0.02050567  1.74412857  1.97655695  2.93276584 -0.19083805\n",
      "  -2.34960527]\n",
      " [ 1.30076617 -0.49289148  2.39787682  1.18388345 -0.67348273 -0.2425781\n",
      "  -0.00924419]]\n",
      "The max of X is: 3.058999863302364; The min of X is: -3.058998181617171\n",
      "The number of samples in train_free is: 1962151\n",
      "The number of samples in test_free is: 490615\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(file_name, usecols=headers_w_collision)\n",
    "\n",
    "# Additional step to preserve the header order when reading data; \n",
    "data = input_data[headers_w_collision].values\n",
    "\n",
    "# np.random.shuffle(data)\n",
    "print(\"The first 5 values from the dataset are: %s\" % data[0:5, :])\n",
    "# X = normalize_data(data[:, :-1], joint_limits)\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "print(\"The first 5 values after normalization are: %s\" % X[0:5, :])\n",
    "\n",
    "print(\"The max of X is: %s; The min of X is: %s\" % (np.max(X), np.min(X),))\n",
    "\n",
    "cutoff = math.ceil(X.shape[0] * 0.8) # 80 percent training data\n",
    "train_X = X[:cutoff, :]\n",
    "test_X = X[cutoff:, :]\n",
    "train_y = y[:cutoff]\n",
    "test_y = y[cutoff:]\n",
    "\n",
    "train_free_y = train_y[train_y == 1]\n",
    "train_free_X = train_X[train_y == 1]\n",
    "\n",
    "print(\"The number of samples in train_free is: %s\" % train_free_X.shape[0])\n",
    "\n",
    "test_free_X = test_X[test_y == 1]\n",
    "test_free_y = test_y[test_y == 1]\n",
    "\n",
    "print(\"The number of samples in test_free is: %s\" % test_free_X.shape[0])\n",
    "\n",
    "train_loader = DataLoader(train_free_X, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_free_X, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "  def __init__(self, d_in, h_dim1, h_dim2, d_out, keep_prob=0):\n",
    "    super(VAE, self).__init__()\n",
    "    # The latent variable is defined as z = N(\\mu, \\sigma).\n",
    "\n",
    "    # Encoder network\n",
    "    self.fc1 = torch.nn.Linear(d_in, h_dim1)\n",
    "    self.fc2 = torch.nn.Linear(h_dim1, h_dim2)\n",
    "    self.mean = torch.nn.Linear(h_dim2, d_out)\n",
    "    self.logvar = torch.nn.Linear(h_dim2, d_out)\n",
    "\n",
    "    # Decoder network\n",
    "    self.fc3 = torch.nn.Linear(d_out, h_dim2)\n",
    "    self.fc4 = torch.nn.Linear(h_dim2, h_dim1)\n",
    "    self.fc5 = torch.nn.Linear(h_dim1, d_in)\n",
    "\n",
    "  # Encode the input into a normal distribution\n",
    "  def encode(self, x):\n",
    "    h1 = F.elu(self.fc1(x))\n",
    "    h2 = F.elu(self.fc2(h1))\n",
    "    return self.mean(h2), self.logvar(h2)\n",
    "\n",
    "  # Reparametrize the normal;\n",
    "  def reparameterize(self, mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "  def decode(self, z):\n",
    "    h2 = F.elu(self.fc3(z))\n",
    "    h1 = F.elu(self.fc4(h2))\n",
    "    # No need to normalize when using mse loss;\n",
    "    # return torch.sigmoid(self.fc5(h1))\n",
    "    return self.fc5(h1)\n",
    "\n",
    "  # Forward pass;\n",
    "  def forward(self, x):\n",
    "    # Change the array to float first;\n",
    "    mu, logvar = self.encode(x.view(-1, num_joints))\n",
    "    # print(\"The value of mu and logvars are: %s, %s\" % (mu, logvar))\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    return self.decode(z), mu, logvar\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function_bce(criterion, recon_x, x, mu, logvar, beta):\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, num_joints), reduction='sum')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * KLD, recon_loss, KLD\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function_mse(recon_x, x, mu, logvar, beta):\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    recon_loss = mse(recon_x, x.view(-1, num_joints))\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * KLD, recon_loss, KLD\n",
    "\n",
    "def train(model, optimizer, device, epoch, train_loader, kld_weight):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  recon_loss = 0\n",
    "  kld_loss = 0\n",
    "  for batch_idx, configs in enumerate(train_loader):\n",
    "    configs = configs.to(device).float()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    recon_batch, mu, logvar = model(configs)\n",
    "    # print(\"Max in the reconstructed_batch: %s; Max in the original dataset: %s\" %  (np.max(recon_batch.detach().numpy()), np.max(configs.detach().numpy()),))\n",
    "    # print(\"Min in the reconstructed_batch: %s; Min in the original dataset: %s\" % (np.min(recon_batch.detach().numpy()), np.min(configs.detach().numpy()),))\n",
    "    \n",
    "    # Customize different cost function here.\n",
    "    # loss, bce, kld = loss_function_bce(recon_batch, configs, mu, logvar)\n",
    "    loss, rec, kld = loss_function_mse(recon_batch, configs, mu, logvar, kld_weight)\n",
    "    \n",
    "    train_loss += loss.item()\n",
    "    recon_loss += rec.item()\n",
    "    kld_loss += kld.item()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print('====> Epoch: {} Average total training_Loss: {:.4f}; Reconstruction loss: {:.4f}; KLD: {:.4f}'.format(\n",
    "    epoch, train_loss / len(train_loader.dataset), \n",
    "    recon_loss / len(train_loader.dataset), \n",
    "    kld_loss / len(train_loader.dataset)))\n",
    "  return mu, logvar, train_loss / len(train_loader.dataset), recon_loss / len(train_loader.dataset), kld_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(model, epoch, device, test_loader, kld_weight):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for i, configs in enumerate(test_loader):\n",
    "        configs = configs.to(device).float()\n",
    "        recon_batch, mu, logvar = model(configs)\n",
    "        # Customize different cost functions here.\n",
    "        # loss, bce, kld = loss_function_bce(recon_batch, configs, mu, logvar)\n",
    "        loss, recon_loss, kld = loss_function_mse(recon_batch, configs, mu, logvar, kld_weight)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Test_loss: {:.4f};'.format(epoch, test_loss))\n",
    "    return test_loss\n",
    "\n",
    "def generate_samples(generated_batch_size, d_output, device, model):\n",
    "  # Generate sampled outputs;\n",
    "  with torch.no_grad():\n",
    "    norms = torch.randn(generated_batch_size, d_output).to(device)\n",
    "    samples = model.decode(norms).cpu().numpy()\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO for writing generated samples to csv file.\n",
    "\n",
    "from pandas import Series\n",
    "import csv\n",
    "\n",
    "def write_samples_to_csv(filename, samples, limits, sample_headers, headers, write_header=True, print_frame=False):\n",
    "    \"\"\"\n",
    "    Write the samples to a csv\n",
    "    \"\"\"    \n",
    "    # recovered_samples = recover_data(samples, limits )\n",
    "    df = pd.DataFrame(data = samples, columns = sample_headers)\n",
    "    \n",
    "    # Fill the rest of the headers\n",
    "    for header in headers:\n",
    "        if header not in sample_headers:\n",
    "            df[header] = Series(np.zeros(samples.shape[0]), index = df.index)\n",
    "\n",
    "    if print_frame:\n",
    "        print(\"The recovered data frame is: %s\" % df)\n",
    "    df.to_csv(file_name, header=write_header, index = False, quoting=csv.QUOTE_NONNUMERIC, mode='a')\n",
    "\n",
    "def test_write_samples_to_csv():\n",
    "    samples = np.array([\n",
    "        [0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "        [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "    ])\n",
    "    file_name = join(\"../data/test_output/\", \"right_\" + str(num_joints) + '.csv')\n",
    "    write_samples_to_csv(file_name, samples, joint_limits, \n",
    "                         headers, HEADERS, print_frame=True)\n",
    "#test_write_samples_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(epochs, total, reconstruction, divergence):\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    plt.plot(epochs, total, color = 'red', label = 'Total loss')\n",
    "    plt.plot(epochs, reconstruction, color = 'blue', label = 'Reconstruction loss')\n",
    "    plt.plot(epochs, divergence, color = 'green', label = 'Divergence Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Training cost functions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average total training_Loss: 6.9421; Reconstruction loss: 3.5117; KLD: 3.4304\n",
      "====> Epoch: 1 Test_loss: 6.8360;\n",
      "====> Epoch: 2 Average total training_Loss: 6.8284; Reconstruction loss: 3.3733; KLD: 3.4551\n",
      "====> Epoch: 2 Test_loss: 6.8340;\n",
      "====> Epoch: 3 Average total training_Loss: 6.8182; Reconstruction loss: 3.3596; KLD: 3.4585\n",
      "====> Epoch: 3 Test_loss: 6.8181;\n",
      "====> Epoch: 4 Average total training_Loss: 6.8158; Reconstruction loss: 3.3534; KLD: 3.4624\n",
      "====> Epoch: 4 Test_loss: 6.8186;\n",
      "====> Epoch: 5 Average total training_Loss: 6.8120; Reconstruction loss: 3.3515; KLD: 3.4605\n",
      "====> Epoch: 5 Test_loss: 6.8065;\n",
      "====> Epoch: 6 Average total training_Loss: 6.8103; Reconstruction loss: 3.3528; KLD: 3.4575\n",
      "====> Epoch: 6 Test_loss: 6.8107;\n",
      "====> Epoch: 7 Average total training_Loss: 6.8054; Reconstruction loss: 3.3484; KLD: 3.4569\n",
      "====> Epoch: 7 Test_loss: 6.8106;\n",
      "====> Epoch: 8 Average total training_Loss: 6.8059; Reconstruction loss: 3.3440; KLD: 3.4619\n",
      "====> Epoch: 8 Test_loss: 6.8101;\n",
      "====> Epoch: 9 Average total training_Loss: 6.8069; Reconstruction loss: 3.3369; KLD: 3.4700\n",
      "====> Epoch: 9 Test_loss: 6.8050;\n",
      "====> Epoch: 10 Average total training_Loss: 6.8033; Reconstruction loss: 3.3367; KLD: 3.4666\n",
      "====> Epoch: 10 Test_loss: 6.8022;\n",
      "Final average test loss: 6.8022;\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(d_input, h_dim1, h_dim2, d_output).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model_path = \"./model\"\n",
    "\n",
    "output_directory = model_path + '/' + str(num_joints) + '-' + str(h_dim1) + '-' + str(h_dim2) + '-' + str(d_output) + '-' + str(batch_size) + '-' + \\\n",
    "          str(learning_rate)\n",
    "if not os.path.exists(output_directory):\n",
    "  os.makedirs(output_directory)\n",
    "\n",
    "training_losses = []\n",
    "test_loss = math.inf\n",
    "for epoch in range(1, epochs + 1):\n",
    "  mu, logvar, train_loss, recon_loss, kld = train(model=model, optimizer=optimizer, device=device, epoch=epoch, \n",
    "                                                  train_loader=train_loader, kld_weight = kld_weight)\n",
    "  training_losses.append((train_loss, recon_loss, kld))\n",
    "  test_loss = test(model=model, epoch=epoch, device=device, test_loader=test_loader, kld_weight = kld_weight)\n",
    "\n",
    "# Print out metric for evaluation.\n",
    "print(\"Final average test loss: {:.4f};\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xt4VOW5/vHvk4QziAIBVKhIpVYNSaAJiiCotHhCpIpFQRSqRRRFZW8Ltv1V66H1dFlLtbLdVtENBS2KurG6rbUW8diAQRBQkYKchIAVxKDk8Pz+mJUwCZNkJpkwYXF/rmtdsw7vWuuZIdyz5p2Zd8zdERGRcElLdQEiIpJ8CncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbsknZmlm9kuM/tWMtseCMzscDNbZGZfmtld+/G8oXocpeEU7kIQChVTuZntjloek+jx3L3M3du6+6fJbJtqZnaFmb1WR7OJwCbgEHef2oi1LDKzcRXLB9LjKPtHRqoLkNRz97YV82a2FrjC3V+pqb2ZZbh76f6o7QB0FLDC9e1ASTFduUudzOx2M3vSzOaY2ZfAJWbW38zeNrMvzGyzmU03s2ZB+wwzczPrESzPCra/GHRXvGVmRyfaNth+lpl9ZGY7zOz3ZvZG9BVstbozzOz/mdknZrbTzArM7Ihg28BgeYeZvWtmJ0btd7mZrQ3Ov8bMLjKz3sADwCnBK5ptMc73P8AY4GdBm1OD+3NLVJvvB0+gFcsbzGyKmS0LapljZi2itp9vZoVB/avNbGjQ3dMfmBGc5/4Yj+OhwbmLgvtyk5lZsO0KM/uHmf02+PdbY2ZDa7v/df2NSBPk7po0VU7AWuD71dbdDuwBziVyQdAKyAdOJPLqryfwEXBN0D4DcKBHsDwL2AbkAc2AJ4FZ9WjbGfgSOC/YNgUoAcbVcF9uApYCvYK6c4EOQCdgB3BxcP5LgO3AYcAhwbZewTEOB44P5q8AXqvj8ZsF3FLL8veBtVHLG4C3ga5Ax+BxvCLYdjLwBTAkqL87cGywbVH0/Y7xOP4JeAZoF/z7rAYui7ofJcCPgXTgWmB9sK3G+6/pwJp05S7xWuTu/+vu5e6+293/6e7vuHupu68BHgYG17L/PHcvcPcSYDaRoE207TCg0N2fC7b9lsgTQU2uAH7m7h8HdRe6++dEnqQ+cPc5Qf2zgDXAOcF+DmSZWUt33+zuK2p/aBrsfnf/zN23AwvYe38vB/7b3f8W1L/e3T+s62DBK6gfAdPc/cvg3+e3wNioZp+4+6PuXgY8DnQzs07Btv19/6URKNwlXuujF8zsu2b2gpl9ZmY7gVuJXBHX5LOo+WKgbU0Na2l7RHQd7u5Ernxr0h34JMb6I4B11datA450951ErugnAZ+Z2QIz+04t50iGmu5vTfXXpTORK/Lo+7gOOLKWcwK0TdH9l0agcJd4VX+D8L+A5cAx7n4I8EvAGrmGzUC3ioWgD/nImpuzHvh2jPWbiLzxGe1bwEYAd3/R3b9PpEtiNZH7Cvs+BvH4Cmgdtdw1gX1rqr+uWrYCZVS9j5X3ry613H85gCjcpb7aEemb/crMjgOu3A/nXAD0NbNzzSwDuA7IrKX9I8DtZvZti8g1sw7BcU4ws1HBG5GjgWOAv1jkc+rnmllrIu8zfEUkKAG2EOm+aJZAzYXAOWZ2mJkdDkxOYN8/AleY2WlmlmZm3czs2KhaesbaKeiymgf82szaBm9I30Ck/79Wddx/OYAo3KW+/gO4jMgbnP9F5I3PRuXuW4BRwH1E3gD9NvAe8E0Nu9wDPAv8DdhJ5H2Blu5eBAwHpgbHuQEYFvTHpwM3EnmVsJ3Im5rXBMf7K/AxsMXMors1ajMTWEmkW+QlYG6c++HubwI/AaYTeSL9O5GuGoD7gYuDT7vcF2P3q4mE87+AfxDpV38ijtPWdv/lAGKRbkuRA4+ZpRPpYhnp7q+nuh6RpkRX7nJAMbMzzax98Fnw/weUAu+muCyRJkfhLgeagUQ+trgNOBMY4e41dcuIHLTULSMiEkK6chcRCaGUDRzWqVMn79GjR6pOLyJyQFq8ePE2d6/tI8BACsO9R48eFBQUpOr0IiIHJDOr/u3qmNQtIyISQgp3EZEQqjPczezYYDzpimmnmV1frY1ZZAzu1Wb2vpn1bbySRUSkLnX2uQdDjOZC5TcCNwLzqzU7i8iY2b2IjPH9UHArIiIpkGi3zBAi40BX79A/D3jCI94GDg0GSRIRkRRINNwvAubEWH8kVcf73kCMoVjNbELw02YFRUVFCZ5aRETiFXe4m1lzIiPp/TnW5hjr9vnqq7s/7O557p6XmVnnxzRFRKSeEvmc+1nAkmDY1eo2sHcoUoj8oMKmhhRWow8/hNmzITcXcnLg6KMhTR/6ERGJlki4X0zsLhmA54FrzGwukTdSd7j75oYWF1NhIdxxB5SXR5bbtoXs7EjQVwR+Vha0adMopxcRORDENXBY8Kss64Ge7r4jWDcRwN1nBD939gCRUfqKgfHuXuvXT/Py8rze31DdvRs++CAS9EuX7p127qwoGHr1qhr4OTlw5JGRbSIiBygzW+zueXW2S9WokA0K91jcYe3aqmG/dCmsWbO3TYcO+wb+8cdD8+bJq0NEpBHFG+4pG1sm6cwi/e9HHw0jRuxdv3MnvP9+1cCfMSNy9Q+QkQHHHVc18HNyQG/4isgBLDxX7okoK4OPP94b9hXdO5ui3gM+/PB9A/8734H09NTULCLCwXjlnoj0dPjudyPTqFF712/bVjXsly6Fv/4VSksj21u2jLxZWxH6vXpFunSaNYu8AqiYElnWewAi0ggOziv3ROzZAytX7vvm7fbtyTl+WlriTwjVl9PSIk8STWGqqCUtrep8ItuSsT9UrSuZy/G2TUuLXEhU1NsYU13Hr37xUP3/e6z//3W1SXQ5el30ttrWNcY+FaL/v8Tzdxh92wQuxnTlnizNm+/tlqngDhs3Rt7ALS3dO5WUJLbc0DZ79kBxcaSbyb3pTOXle2+j52NtEznQ1BT6iTxRXHMN/OxnjVqmwr0+zKBbt8gkDVPTk0K8Tw41PWFEHzeZy4m0rainsaaysvjaVb/arGs5njYNOWb0ttrWNcY+1f/Wavrbqs9tIm2PPXbfxyfJFO6SWtEvdfVmtUjS6Hv7IiIhpHAXEQkhhbuISAgp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIYW7iEgIxRXuZnaomc0zs1VmttLM+lfbfqqZ7TCzwmD6ZeOUKyIi8Yh34LDfAS+5+0gzaw60jtHmdXcflrzSRESkvuoMdzM7BBgEjANw9z3AnsYtS0REGiKebpmeQBHwmJm9Z2aPmFmbGO36m9lSM3vRzE6IdSAzm2BmBWZWUFRU1JC6RUSkFvGEewbQF3jI3fsAXwHTqrVZAhzl7jnA74FnYx3I3R929zx3z8vMzGxA2SIiUpt4wn0DsMHd3wmW5xEJ+0ruvtPddwXzfwGamVmnpFYqIiJxqzPc3f0zYL2ZVfwu1BBgRXQbM+tqFvk5HTPrFxw3Sb8gLSIiiYr30zLXArODT8qsAcab2UQAd58BjASuMrNSYDdwkXusn0AXEZH9wVKVwXl5eV5QUJCSc4uIHKjMbLG759XVTt9QFREJIYW7iEgIKdxFREJI4S4iEkIKdxGREFK4i4iEkMJdRCSEFO4iIiGkcBcRCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbuISAgp3EVEQiiucDezQ81snpmtMrOVZta/2nYzs+lmttrM3jezvo1TroiIxCMjzna/A15y95Fm1hxoXW37WUCvYDoReCi4FRGRFKjzyt3MDgEGAX8EcPc97v5FtWbnAU94xNvAoWZ2eNKrFRGRuMTTLdMTKAIeM7P3zOwRM2tTrc2RwPqo5Q3BuirMbIKZFZhZQVFRUb2LFhGR2sUT7hlAX+Ahd+8DfAVMq9bGYuzn+6xwf9jd89w9LzMzM+FiRUQkPvGE+wZgg7u/EyzPIxL21dt0j1ruBmxqeHkiIlIfdYa7u38GrDezY4NVQ4AV1Zo9D1wafGrmJGCHu29ObqkiIhKveD8tcy0wO/ikzBpgvJlNBHD3GcBfgLOB1UAxML4RahURkTjFFe7uXgjkVVs9I2q7A5OSWJeIiDSAvqEqIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbuISAgp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIYW7iEgIKdxFREJI4S4iEkLx/syeiDSikpISNmzYwNdff53qUqSJaNmyJd26daNZs2b12j+ucDeztcCXQBlQ6u551bafCjwH/CtY9Yy731qvikQOQhs2bKBdu3b06NEDM0t1OZJi7s727dvZsGEDRx99dL2OkciV+2nuvq2W7a+7+7B6VSFykPv6668V7FLJzOjYsSNFRUX1Pob63EWaCAW7RGvo30O84e7Ay2a22Mwm1NCmv5ktNbMXzeyEWA3MbIKZFZhZQUOekUREpHbxhvsAd+8LnAVMMrNB1bYvAY5y9xzg98CzsQ7i7g+7e56752VmZta7aBFJru3bt5Obm0tubi5du3blyCOPrFzes2fPPu0///xzZsyYUedxS0tLOfTQQ+NeL8kTV7i7+6bgdiswH+hXbftOd98VzP8FaGZmnZJcq4g0ko4dO1JYWEhhYSETJ07khhtuqFxu3rz5Pu3jDXdJnTrD3czamFm7inlgKLC8WpuuFnQQmVm/4Ljbk1+uiOxvd999N1lZWWRlZfH73/8egGnTpvHhhx+Sm5vLtGnT2LlzJ6effjp9+/YlOzubBQsWxH388vJypkyZQlZWFr1792bevHkAbNy4kYEDB5Kbm0tWVhZvvvkmpaWljB07lt69e5OVlcX06dMb5T6HQTyflukCzA+yOwP4k7u/ZGYTAdx9BjASuMrMSoHdwEXu7o1Us0i4XX89FBYm95i5uXD//Qnv9u677zJ79mzeffddysrK6NevH4MHD+bOO+9k9erVFAZ1lpSU8Nxzz9GuXTu2bt3KgAEDGDYsvg/P/fnPf2bFihUsXbqUoqIi8vPzGTRoELNmzeLcc89l6tSplJWVsXv3bhYvXsy2bdtYtmwZAF988UXC9+lgUWe4u/saICfG+hlR8w8ADyS3NBFJtddff50LLriA1q1bAzBixAgWLVrE0KFDq7Rzd6ZOncqiRYtIS0tj/fr1bNu2La5+9UWLFjF69GjS09Pp2rUrAwcOpKCggPz8fK688kq+/vprRowYQU5ODscccwwffvgh1113HWefffY+dche+oaqSFNTjyvsxhLvC/AnnniCHTt2sGTJEjIyMujWrVvc37at6Rynn346r732Gi+88AJjxozhpptuYsyYMbz//vu8+OKLTJ8+naeffpqHH3447vtzMNHn3EWkRoMGDWL+/Pns3r2bXbt28dxzz3HKKafQrl07vvzyy8p2O3bsoHPnzmRkZPDXv/6VjRs3JnSOuXPnUlZWxpYtW3jjjTfIy8tj3bp1dO3alQkTJjBu3Djee+89ioqKcHcuvPBCfvWrX7FkyZLGuNuhoCt3EalRv379uPjii8nPzwfgqquuonfv3gDk5eXRu3dvzjnnHKZMmcK5555LXl4effv2pVevXnGfY+TIkbz99tvk5ORgZtx333107tyZRx99lPvuu49mzZrRtm1bZs2axfr167n88stxd8yMu+66q1HudxhYqt73zMvL84KCgpScW6SpWblyJccdd1yqy5AmJtbfhZktrj6+VyzqlhERCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQASE9Prxyk69xzz035uC2//vWvk3asL774gj/84Q+Vy5s2bWLkyJFJOfapp55KU/xYt8JdRABo1aoVhYWFLF++nA4dOvDggw+mtJ6awt3dKS8vT+hY1cP9iCOOqBx9MqwU7iKyj/79+1cZQuCee+4hPz+f7Oxsbr755sr1TzzxBNnZ2eTk5DB27FgA1q1bx5AhQ8jOzmbIkCF8+umnAIwbN47Jkydz8skn07Nnz8pw3bx5M4MGDap81fD6668zbdo0du/eTW5uLmPGjGHt2rUcd9xxXH311fTt25f169fTtm3byjrmzZvHuHHjANiyZQs//OEPycnJIScnhzfffJNp06bxySefkJuby4033sjatWvJysoCIr9fO378eHr37k2fPn34+9//DsDMmTM5//zzOfPMM+nVqxc//elP63zc5syZUzkc8dSpUwEoKytj3LhxlUMa//a3vwVg+vTpHH/88WRnZ3PRRRfV69+pNhp+QKSJSfWIv2VlZfztb3/j8ssvB+Dll1/m448/5t1338XdGT58OAsXLqRjx47ccccdvPHGG3Tq1InPP/8cgGuuuYZLL72Uyy67jEcffZTJkyfz7LORH2fbvHkzixYtYtWqVQwfPpyRI0fypz/9iTPOOIOf//znlJWVUVxczCmnnMIDDzxQOaTw2rVr+fDDD3nssceqXIHHMnnyZAYPHsz8+fMpKytj165d3HnnnSxfvrzK8SpUvEJZtmwZq1atYujQoXz00UcAFBYW8t5779GiRQuOPfZYrr32Wrp37x7zvJs2bWLq1KksXryYww47jKFDh/Lss8/SvXt3Nm7cyPLlkZ/BqOjuuvPOO/nXv/5FixYtGqULTFfuIgJQeaXcsWNHPv/8c37wgx8AkXB/+eWX6dOnD3379mXVqlV8/PHHvPrqq4wcOZJOnSI/utahQwcA3nrrLUaPHg3A2LFjWbRoUeU5RowYQVpaGscffzxbtmwBID8/n8cee4xbbrmFZcuW0a5du5j1HXXUUZx00kl13o9XX32Vq666Coi8j9C+ffta2y9atKjyVcd3v/tdjjrqqMpwHzJkCO3bt6dly5Ycf/zxrFu3rsbj/POf/+TUU08lMzOTjIwMxowZw8KFC+nZsydr1qzh2muv5aWXXuKQQw4BIDs7mzFjxjBr1iwyMpJ/na0rd5EmJlUj/lb0ue/YsYNhw4bx4IMPMnnyZNydm266iSuvvLJK++nTpxP8iE+totu0aNGicr5iXKtBgwaxcOFCXnjhBcaOHcuNN97IpZdeus9x2rRpU+Nx4x1eOJbaxteKrjc9PZ3S0tKEj3PYYYexdOlS/u///o8HH3yQp556ikcffZQXXniBhQsX8vzzz3PbbbfxwQcfJDXkdeUuIlW0b9+e6dOnc++991JSUsIZZ5zBo48+yq5du4DIz99t3bqVIUOG8NRTT7F9e+QXNSu6ZU4++WTmzp0LwOzZsxk4cGCt51u3bh2dO3fmJz/5CZdffnnlML7NmjWjpKSkxv26dOnCypUrKS8vZ/78+ZXrhwwZwkMPPQREuph27ty5zxDF0QYNGsTs2bMB+Oijj/j000859thj63ycqjvxxBP5xz/+wbZt2ygrK2POnDkMHjyYbdu2UV5ezgUXXMBtt93GkiVLKC8vZ/369Zx22mncfffdfPHFF5WPb7Loyl1E9tGnTx9ycnKYO3cuY8eOZeXKlfTv3x+gcvjdE044gZ///OcMHjyY9PR0+vTpw8yZM5k+fTo//vGPueeee8jMzOSxxx6r9VyvvfYa99xzT+XQvk888QQAEyZMIDs7m759+3LHHXfss9+dd97JsGHD6N69O1lZWZXh+Lvf/Y4JEybwxz/+kfT0dB566CH69+/PgAEDyMrK4qyzzmLSpEmVx7n66quZOHEivXv3JiMjg5kzZ1a5Yo/X4Ycfzm9+8xtOO+003J2zzz6b8847j6VLlzJ+/PjKT/j85je/oaysjEsuuYQdO3bg7txwww1x/WpVIjTkr0gToCF/JZZGH/LXzNaa2TIzKzSzfRLZIqab2Woze9/M+sZdvYiIJF0i3TKnufu2GradBfQKphOBh4JbERFJgWS9oXoe8IRHvA0camaHJ+nYIiKSoHjD3YGXzWyxmU2Isf1IYH3U8oZgXRVmNsHMCsysoKioKPFqRUQkLvGG+wB370uk+2WSmQ2qtj3Wh133eafW3R929zx3z8vMzEywVBERiVdc4e7um4LbrcB8oF+1JhuA6O/kdgM2JaNAERFJXJ3hbmZtzKxdxTwwFFherdnzwKXBp2ZOAna4++akVysijaZiyN8TTjiBnJwc7rvvvsrPZhcUFDB58uQUV5hc48aNC/XIkPF8WqYLMD/4qm8G8Cd3f8nMJgK4+wzgL8DZwGqgGBjfOOWKSGOpGH4AYOvWrYwePZodO3bwq1/9iry8PPLy6vxodZ1KS0sbZRwV2VedV+7uvsbdc4LpBHe/I1g/Iwh2gk/JTHL3b7t7b3fXt5NEDmCdO3fm4Ycf5oEHHsDdee211xg2bBjl5eX06NGjyiiGxxxzDFu2bKGoqIgLLriA/Px88vPzeeONNwC45ZZbmDBhAkOHDuXSSy+luLiYH/3oR2RnZzNq1ChOPPHEyh+7ePnll+nfvz99+/blwgsvrPzWaY8ePbj55pvp27cvvXv3ZtWqVQDs2rWrcrje7Oxsnn766VqPUxd358Ybb6wcnvfJJ58EYg9LXNNQvk2FnkJFmpjrX7qews+SO+Zvbtdc7j8zsRHJevbsSXl5OVu3bq1cl5aWxnnnncf8+fMZP34877zzDj169KBLly6MHj2aG264gYEDB/Lpp59yxhlnsHLlSgAWL17MokWLaNWqFffeey+HHXYY77//PsuXLyc3NxeAbdu2cfvtt/PKK6/Qpk0b7rrrLu677z5++ctfAtCpUyeWLFnCH/7wB+69914eeeQRbrvtNtq3b8+yZcsA+Pe//13ncWrzzDPPUFhYyNKlS9m2bRv5+fkMGjQo5rDEhYWFMYfybSoU7iJSo1jDk4waNYpbb72V8ePHM3fuXEaNGgXAK6+8wooVKyrb7dy5s3KwruHDh9OqVSsgMsTuddddB0BWVhbZ2dkAvP3226xYsYIBAwYAsGfPnsrxbADOP/98AL73ve/xzDPPVJ6zYpAyiIzAuGDBglqPU5tFixZx8cUXk56eTpcuXRg8eDD//Oc/yc/P58c//jElJSWMGDGC3NzcKkP5nnPOOQwdOjSuc+wvCneRJibRK+zGsmbNGtLT0+ncuXPlFThEfqVp9erVFBUV8eyzz/KLX/wCgPLyct56663KEI8WPVxvTeNZuTs/+MEPmDNnTsztFYN5RQ+96+77DDtc13FqU1NtNQ1LHGso36ZCQ/6KyD6KioqYOHEi11xzzT7haWb88Ic/ZMqUKRx33HF07NgRgKFDh/LAAw9Utius4eekBg4cyFNPPQXAihUrKrtUTjrpJN544w1Wr14NQHFxceWPZtSk+jn//e9/1+s4FQYNGsSTTz5JWVkZRUVFLFy4kH79+sUcljjWUL5Nia7cRQTY+0tMJSUlZGRkMHbsWKZMmRKz7ahRo8jPz2fmzJmV66ZPn86kSZPIzs6mtLSUQYMGMWPGjH32vfrqq7nsssvIzs6mT58+ZGdn0759ezIzM5k5cyYXX3wx33zzDQC333473/nOd2qs+Re/+AWTJk0iKyuL9PR0br75Zs4///y4j3PllVdy/fXXA9C9e3fefPNN3nrrLXJycjAz7r77brp27crjjz++z7DEGzdu3Gco36ZEQ/6KNAEH05C/ZWVllJSU0LJlSz755BOGDBnCRx99RPPmzVNdWpPTkCF/deUuIvtVcXExp512GiUlJbg7Dz30kIK9ESjcRWS/ateuHXrV3vj0hqpIE5GqLlJpmhr696BwF2kCWrZsyfbt2xXwAkSCffv27bRs2bLex1C3jEgT0K1bNzZs2IB+50AqtGzZkm7dutV7f4W7SBPQrFkzjj766FSXISGibhkRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbuISAjFHe5mlm5m75nZghjbxplZkZkVBtMVyS1TREQSkcjn3K8DVgKH1LD9SXe/puEliYhIQ8V15W5m3YBzgEcatxwREUmGeLtl7gd+CpTX0uYCM3vfzOaZWfdYDcxsgpkVmFmBvmYtItJ46gx3MxsGbHX3xbU0+1+gh7tnA68Aj8dq5O4Pu3ueu+dlZmbWq2AREalbPFfuA4DhZrYWmAucbmazohu4+3Z3/yZY/G/ge0mtUkREElJnuLv7Te7ezd17ABcBr7r7JdFtzOzwqMXhRN54FRGRFKn3qJBmditQ4O7PA5PNbDhQCnwOjEtOeSIiUh/6gWwRkQNIvD+QrW+oioiEkMJdRCSEFO4iIiGkcBcRCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbuISAgp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJITiDnczSzez98xsQYxtLczsSTNbbWbvmFmPZBYpIiKJSeTK/TpgZQ3bLgf+7e7HAL8F7mpoYSIiUn9xhbuZdQPOAR6pocl5wOPB/DxgiJlZw8sTEZH6iPfK/X7gp0B5DduPBNYDuHspsAPo2ODqRESkXuoMdzMbBmx198W1NYuxzmMca4KZFZhZQVFRUQJliohIIuK5ch8ADDeztcBc4HQzm1WtzQagO4CZZQDtgc+rH8jdH3b3PHfPy8zMbFDhIiJSszrD3d1vcvdu7t4DuAh41d0vqdbseeCyYH5k0GafK3cREdk/Muq7o5ndChS4+/PAH4H/MbPVRK7YL0pSfSIiUg8Jhbu7vwa8Fsz/Mmr918CFySxMRETqT99QFREJIYW7iEgIKdxFREJI4S4iEkIKdxGREFK4i4iEkMJdRCSEFO4iIiGkcBcRCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbuISAgp3EVEQqjOcDezlmb2rpktNbMPzOxXMdqMM7MiMysMpisap1wREYlHPD+Q/Q1wurvvMrNmwCIze9Hd367W7kl3vyb5JVa1bh089xz07w+5udCsWWOfUUTkwFNnuLu7A7uCxWbB5I1ZVG0WLoTrrovMt2oFeXmRoD/55Mht586pqkwLDa2HAAAHoElEQVREpOmwSHbX0cgsHVgMHAM86O5Tq20fB/wGKAI+Am5w9/UxjjMBmADwrW9963vr1q2rV9EbNsBbb8Gbb0ZulyyBkpLItm9/u2rYZ2VBRjyvT0TkgFTu5ZSVl1FaXkqZl9U67ziGkWZpmFmV+TRLw7Aq8/G2i7VPYzGzxe6eV2e7eMI96qCHAvOBa919edT6jsAud//GzCYCP3L302s7Vl5enhcUFMR97trs3h0J+Iqwf/NN2LIlsq1tW+jXb2/Yn3QSdOgQ33FLykooLimunL4q+arK8telX1Pu5bg7jlfOl3t5leVY22K1q21bPMeP3h7r2NHrYh0rru21HL9CxX+Gij/wivma1jXWPmmkVV2O+o9Y/T9urO312af69mgVdUc/TvFsa+i+Ff9eNU1l5WV1tom5n8e/X12hm8h8aXkpZeVllfNNWU1PEv958n9y62m31uuY8YZ7Qte07v6Fmb0GnAksj1q/ParZfwN3JXLc+qgevO2O+YpTjirmeyOLGb+nmLUbv2LZh8V8+Ekxqz4t5u9vfIW/UwzNimnfqZgOXYpp1/ErWrUrxpoXU1xazFd7qoZ3SXlJY9+NBqsIkXRLrwyb6n9Q1ddVvwKpvq4++1f84VY8OXnQc1cxX9M6oFH2qf6EWP1JM97tNe0TNhX/rvWZ0i09rnYZaRmkp6WTbulkpGXQIqNF5Xz0+irzlk56Wuz5WveLMZ+ell7lbzTRi6m62iWyz0ndTmr0f9M6w93MMoGSINhbAd+nWnib2eHuvjlYHA6sTHqlgXkr5jH66dHxB2874ITIbPO0lqSXt+abb9qw7qvWlP+rNZS0JsPb0aFdV7p0aM13OrfmqCNac1jbNrRu1rpyatOs2nLzNrRIb1Fr0CVrW03tGvOln9StrieI6m2rLEdtr21bMvZNT6s9fPW3FE7xXLkfDjwe9LunAU+5+wIzuxUocPfngclmNhwoBT4HxjVWwcd2PJb/PPk/6wze6ttbNWtFmu395Kc7fPxxVFfOq7D8A1jmYBbpq6/oyunfH3r1iqwXqRD9ZCzS1CTU555MyexzT5YdO+CddyJhXzHt3BnZ1qlTpL++IvDz86FNm9TWKyIHn0bpcw+79u1h6NDIBFBeDitXVn2jdsGCyLb09Mgnc1q1gubNI5+3b968cebr2p6REaknLS1yGz1fcWumVx4iBxOFey3S0uCEEyLTT34SWbd9O7z9diTsP/oI9uyJTCUlkdudO/fOR6+PNZ+K+xMr+GOtq8+2tKB3ouKJJPoJZX+ug0i3W1238bSp774V9VQ8Lmlp+y7vr3VNZX3F30y8U1376GKldgr3BHXsCOecE5kawh1KS+t+AqiYr21baSmUlUVeacS63V/bKkKvvHzfAIyeGntddNjXdZtI20T3rXgsoqf9se5gYVa/J4xEz9EY7a+4AqZMSezYiVK4p4hZpFtFwydIskU/yVY86db0RLC/1ldcECQyRV9IJGtK5C3GRN+OTKR9ly6JHbs+FO4iIVPxqiItTd/OPpjpM1wiIiGkcBcRCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhFI2KqSZFQH1+529pqMTsC3VRTQhejyq0uOxlx6LqhryeBzl7pl1NUpZuIeBmRXEM/TmwUKPR1V6PPbSY1HV/ng81C0jIhJCCncRkRBSuDfMw6kuoInR41GVHo+99FhU1eiPh/rcRURCSFfuIiIhpHAXEQkhhXs9mFl3M/u7ma00sw/M7LpU15RqZpZuZu+Z2YJU15JqZnaomc0zs1XB30j/VNeUSmZ2Q/D/ZLmZzTGzlqmuaX8ys0fNbKuZLY9a18HM/mpmHwe3hyX7vAr3+ikF/sPdjwNOAiaZ2fEprinVrgNWprqIJuJ3wEvu/l0gh4P4cTGzI4HJQJ67ZwHpwEWprWq/mwmcWW3dNOBv7t4L+FuwnFQK93pw983uviSY/5LIf94jU1tV6phZN+Ac4JFU15JqZnYIMAj4I4C773H3L1JbVcplAK3MLANoDWxKcT37lbsvBD6vtvo84PFg/nFgRLLPq3BvIDPrAfQB3kltJSl1P/BToDzVhTQBPYEi4LGgm+oRM2uT6qJSxd03AvcCnwKbgR3u/nJqq2oSurj7ZohcLAKdk30ChXsDmFlb4Gngenffmep6UsHMhgFb3X1xqmtpIjKAvsBD7t4H+IpGeMl9oAj6ks8DjgaOANqY2SWprergoHCvJzNrRiTYZ7v7M6muJ4UGAMPNbC0wFzjdzGaltqSU2gBscPeKV3LziIT9wer7wL/cvcjdS4BngJNTXFNTsMXMDgcIbrcm+wQK93owMyPSp7rS3e9LdT2p5O43uXs3d+9B5I2yV939oL0yc/fPgPVmdmywagiwIoUlpdqnwElm1jr4fzOEg/gN5ijPA5cF85cBzyX7BBnJPuBBYgAwFlhmZoXBup+5+19SWJM0HdcCs82sObAGGJ/ielLG3d8xs3nAEiKfMnuPg2woAjObA5wKdDKzDcDNwJ3AU2Z2OZEnwAuTfl4NPyAiEj7qlhERCSGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhP4/kHlXwTcBNjcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_loss, recon_loss, kld_loss = zip(*training_losses)\n",
    "t = range(1, epochs + 1)\n",
    "plot_loss(t, total_loss, recon_loss, kld_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing samples to: ../data/output/right_7_1000000.csv\n",
      "Writing the 0th batch\n",
      "Writing the 1th batch\n",
      "Writing the 2th batch\n",
      "Writing the 3th batch\n",
      "Writing the 4th batch\n",
      "Writing the 5th batch\n",
      "Writing the 6th batch\n",
      "Writing the 7th batch\n",
      "Writing the 8th batch\n",
      "Writing the 9th batch\n",
      "Writing the 10th batch\n",
      "Writing the 11th batch\n",
      "Writing the 12th batch\n",
      "Writing the 13th batch\n",
      "Writing the 14th batch\n",
      "Writing the 15th batch\n",
      "Writing the 16th batch\n",
      "Writing the 17th batch\n",
      "Writing the 18th batch\n",
      "Writing the 19th batch\n",
      "Writing the 20th batch\n",
      "Writing the 21th batch\n",
      "Writing the 22th batch\n",
      "Writing the 23th batch\n",
      "Writing the 24th batch\n",
      "Writing the 25th batch\n",
      "Writing the 26th batch\n",
      "Writing the 27th batch\n",
      "Writing the 28th batch\n",
      "Writing the 29th batch\n",
      "Writing the 30th batch\n",
      "Writing the 31th batch\n",
      "Writing the 32th batch\n",
      "Writing the 33th batch\n",
      "Writing the 34th batch\n",
      "Writing the 35th batch\n",
      "Writing the 36th batch\n",
      "Writing the 37th batch\n",
      "Writing the 38th batch\n",
      "Writing the 39th batch\n",
      "Writing the 40th batch\n",
      "Writing the 41th batch\n",
      "Writing the 42th batch\n",
      "Writing the 43th batch\n",
      "Writing the 44th batch\n",
      "Writing the 45th batch\n",
      "Writing the 46th batch\n",
      "Writing the 47th batch\n",
      "Writing the 48th batch\n",
      "Writing the 49th batch\n",
      "Writing the 50th batch\n",
      "Writing the 51th batch\n",
      "Writing the 52th batch\n",
      "Writing the 53th batch\n",
      "Writing the 54th batch\n",
      "Writing the 55th batch\n",
      "Writing the 56th batch\n",
      "Writing the 57th batch\n",
      "Writing the 58th batch\n",
      "Writing the 59th batch\n",
      "Writing the 60th batch\n",
      "Writing the 61th batch\n",
      "Writing the 62th batch\n",
      "Writing the 63th batch\n",
      "Writing the 64th batch\n",
      "Writing the 65th batch\n",
      "Writing the 66th batch\n",
      "Writing the 67th batch\n",
      "Writing the 68th batch\n",
      "Writing the 69th batch\n",
      "Writing the 70th batch\n",
      "Writing the 71th batch\n",
      "Writing the 72th batch\n",
      "Writing the 73th batch\n",
      "Writing the 74th batch\n",
      "Writing the 75th batch\n",
      "Writing the 76th batch\n",
      "Writing the 77th batch\n",
      "Writing the 78th batch\n",
      "Writing the 79th batch\n",
      "Writing the 80th batch\n",
      "Writing the 81th batch\n",
      "Writing the 82th batch\n",
      "Writing the 83th batch\n",
      "Writing the 84th batch\n",
      "Writing the 85th batch\n",
      "Writing the 86th batch\n",
      "Writing the 87th batch\n",
      "Writing the 88th batch\n",
      "Writing the 89th batch\n",
      "Writing the 90th batch\n",
      "Writing the 91th batch\n",
      "Writing the 92th batch\n",
      "Writing the 93th batch\n",
      "Writing the 94th batch\n",
      "Writing the 95th batch\n",
      "Writing the 96th batch\n",
      "Writing the 97th batch\n",
      "Writing the 98th batch\n",
      "Writing the 99th batch\n",
      "Writing the 100th batch\n",
      "Writing the 101th batch\n",
      "Writing the 102th batch\n",
      "Writing the 103th batch\n",
      "Writing the 104th batch\n",
      "Writing the 105th batch\n",
      "Writing the 106th batch\n",
      "Writing the 107th batch\n",
      "Writing the 108th batch\n",
      "Writing the 109th batch\n",
      "Writing the 110th batch\n",
      "Writing the 111th batch\n",
      "Writing the 112th batch\n",
      "Writing the 113th batch\n",
      "Writing the 114th batch\n",
      "Writing the 115th batch\n",
      "Writing the 116th batch\n",
      "Writing the 117th batch\n",
      "Writing the 118th batch\n",
      "Writing the 119th batch\n",
      "Writing the 120th batch\n",
      "Writing the 121th batch\n",
      "Writing the 122th batch\n",
      "Writing the 123th batch\n",
      "Writing the 124th batch\n",
      "Writing the 125th batch\n",
      "Writing the 126th batch\n",
      "Writing the 127th batch\n",
      "Writing the 128th batch\n",
      "Writing the 129th batch\n",
      "Writing the 130th batch\n",
      "Writing the 131th batch\n",
      "Writing the 132th batch\n",
      "Writing the 133th batch\n",
      "Writing the 134th batch\n",
      "Writing the 135th batch\n",
      "Writing the 136th batch\n",
      "Writing the 137th batch\n",
      "Writing the 138th batch\n",
      "Writing the 139th batch\n",
      "Writing the 140th batch\n",
      "Writing the 141th batch\n",
      "Writing the 142th batch\n",
      "Writing the 143th batch\n",
      "Writing the 144th batch\n",
      "Writing the 145th batch\n",
      "Writing the 146th batch\n",
      "Writing the 147th batch\n",
      "Writing the 148th batch\n",
      "Writing the 149th batch\n",
      "Writing the 150th batch\n",
      "Writing the 151th batch\n",
      "Writing the 152th batch\n",
      "Writing the 153th batch\n",
      "Writing the 154th batch\n",
      "Writing the 155th batch\n",
      "Writing the 156th batch\n",
      "Writing the 157th batch\n",
      "Writing the 158th batch\n",
      "Writing the 159th batch\n",
      "Writing the 160th batch\n",
      "Writing the 161th batch\n",
      "Writing the 162th batch\n",
      "Writing the 163th batch\n",
      "Writing the 164th batch\n",
      "Writing the 165th batch\n",
      "Writing the 166th batch\n",
      "Writing the 167th batch\n",
      "Writing the 168th batch\n",
      "Writing the 169th batch\n",
      "Writing the 170th batch\n",
      "Writing the 171th batch\n",
      "Writing the 172th batch\n",
      "Writing the 173th batch\n",
      "Writing the 174th batch\n",
      "Writing the 175th batch\n",
      "Writing the 176th batch\n",
      "Writing the 177th batch\n",
      "Writing the 178th batch\n",
      "Writing the 179th batch\n",
      "Writing the 180th batch\n",
      "Writing the 181th batch\n",
      "Writing the 182th batch\n",
      "Writing the 183th batch\n",
      "Writing the 184th batch\n",
      "Writing the 185th batch\n",
      "Writing the 186th batch\n",
      "Writing the 187th batch\n",
      "Writing the 188th batch\n",
      "Writing the 189th batch\n",
      "Writing the 190th batch\n",
      "Writing the 191th batch\n",
      "Writing the 192th batch\n",
      "Writing the 193th batch\n",
      "Writing the 194th batch\n",
      "Writing the 195th batch\n",
      "Writing the 196th batch\n",
      "Writing the 197th batch\n",
      "Writing the 198th batch\n",
      "Writing the 199th batch\n",
      "Writing the 200th batch\n",
      "Writing the 201th batch\n",
      "Writing the 202th batch\n",
      "Writing the 203th batch\n",
      "Writing the 204th batch\n",
      "Writing the 205th batch\n",
      "Writing the 206th batch\n",
      "Writing the 207th batch\n",
      "Writing the 208th batch\n",
      "Writing the 209th batch\n",
      "Writing the 210th batch\n",
      "Writing the 211th batch\n",
      "Writing the 212th batch\n",
      "Writing the 213th batch\n",
      "Writing the 214th batch\n",
      "Writing the 215th batch\n",
      "Writing the 216th batch\n",
      "Writing the 217th batch\n",
      "Writing the 218th batch\n",
      "Writing the 219th batch\n",
      "Writing the 220th batch\n",
      "Writing the 221th batch\n",
      "Writing the 222th batch\n",
      "Writing the 223th batch\n",
      "Writing the 224th batch\n",
      "Writing the 225th batch\n",
      "Writing the 226th batch\n",
      "Writing the 227th batch\n",
      "Writing the 228th batch\n",
      "Writing the 229th batch\n",
      "Writing the 230th batch\n",
      "Writing the 231th batch\n",
      "Writing the 232th batch\n",
      "Writing the 233th batch\n",
      "Writing the 234th batch\n",
      "Writing the 235th batch\n",
      "Writing the 236th batch\n",
      "Writing the 237th batch\n",
      "Writing the 238th batch\n",
      "Writing the 239th batch\n",
      "Writing the 240th batch\n",
      "Writing the 241th batch\n",
      "Writing the 242th batch\n",
      "Writing the 243th batch\n",
      "Writing the 244th batch\n",
      "Writing the 245th batch\n",
      "Writing the 246th batch\n",
      "Writing the 247th batch\n",
      "Writing the 248th batch\n",
      "Writing the 249th batch\n",
      "Writing the 250th batch\n",
      "Writing the 251th batch\n",
      "Writing the 252th batch\n",
      "Writing the 253th batch\n",
      "Writing the 254th batch\n",
      "Writing the 255th batch\n",
      "Writing the 256th batch\n",
      "Writing the 257th batch\n",
      "Writing the 258th batch\n",
      "Writing the 259th batch\n",
      "Writing the 260th batch\n",
      "Writing the 261th batch\n",
      "Writing the 262th batch\n",
      "Writing the 263th batch\n",
      "Writing the 264th batch\n",
      "Writing the 265th batch\n",
      "Writing the 266th batch\n",
      "Writing the 267th batch\n",
      "Writing the 268th batch\n",
      "Writing the 269th batch\n",
      "Writing the 270th batch\n",
      "Writing the 271th batch\n",
      "Writing the 272th batch\n",
      "Writing the 273th batch\n",
      "Writing the 274th batch\n",
      "Writing the 275th batch\n",
      "Writing the 276th batch\n",
      "Writing the 277th batch\n",
      "Writing the 278th batch\n",
      "Writing the 279th batch\n",
      "Writing the 280th batch\n",
      "Writing the 281th batch\n",
      "Writing the 282th batch\n",
      "Writing the 283th batch\n",
      "Writing the 284th batch\n",
      "Writing the 285th batch\n",
      "Writing the 286th batch\n",
      "Writing the 287th batch\n",
      "Writing the 288th batch\n",
      "Writing the 289th batch\n",
      "Writing the 290th batch\n",
      "Writing the 291th batch\n",
      "Writing the 292th batch\n",
      "Writing the 293th batch\n",
      "Writing the 294th batch\n",
      "Writing the 295th batch\n",
      "Writing the 296th batch\n",
      "Writing the 297th batch\n",
      "Writing the 298th batch\n",
      "Writing the 299th batch\n",
      "Writing the 300th batch\n",
      "Writing the 301th batch\n",
      "Writing the 302th batch\n",
      "Writing the 303th batch\n",
      "Writing the 304th batch\n",
      "Writing the 305th batch\n",
      "Writing the 306th batch\n",
      "Writing the 307th batch\n",
      "Writing the 308th batch\n",
      "Writing the 309th batch\n",
      "Writing the 310th batch\n",
      "Writing the 311th batch\n",
      "Writing the 312th batch\n",
      "Writing the 313th batch\n",
      "Writing the 314th batch\n",
      "Writing the 315th batch\n",
      "Writing the 316th batch\n",
      "Writing the 317th batch\n",
      "Writing the 318th batch\n",
      "Writing the 319th batch\n",
      "Writing the 320th batch\n",
      "Writing the 321th batch\n",
      "Writing the 322th batch\n",
      "Writing the 323th batch\n",
      "Writing the 324th batch\n",
      "Writing the 325th batch\n",
      "Writing the 326th batch\n",
      "Writing the 327th batch\n",
      "Writing the 328th batch\n",
      "Writing the 329th batch\n",
      "Writing the 330th batch\n",
      "Writing the 331th batch\n",
      "Writing the 332th batch\n",
      "Writing the 333th batch\n",
      "Writing the 334th batch\n",
      "Writing the 335th batch\n",
      "Writing the 336th batch\n",
      "Writing the 337th batch\n",
      "Writing the 338th batch\n",
      "Writing the 339th batch\n",
      "Writing the 340th batch\n",
      "Writing the 341th batch\n",
      "Writing the 342th batch\n",
      "Writing the 343th batch\n",
      "Writing the 344th batch\n",
      "Writing the 345th batch\n",
      "Writing the 346th batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the 347th batch\n",
      "Writing the 348th batch\n",
      "Writing the 349th batch\n",
      "Writing the 350th batch\n",
      "Writing the 351th batch\n",
      "Writing the 352th batch\n",
      "Writing the 353th batch\n",
      "Writing the 354th batch\n",
      "Writing the 355th batch\n",
      "Writing the 356th batch\n",
      "Writing the 357th batch\n",
      "Writing the 358th batch\n",
      "Writing the 359th batch\n",
      "Writing the 360th batch\n",
      "Writing the 361th batch\n",
      "Writing the 362th batch\n",
      "Writing the 363th batch\n",
      "Writing the 364th batch\n",
      "Writing the 365th batch\n",
      "Writing the 366th batch\n",
      "Writing the 367th batch\n",
      "Writing the 368th batch\n",
      "Writing the 369th batch\n",
      "Writing the 370th batch\n",
      "Writing the 371th batch\n",
      "Writing the 372th batch\n",
      "Writing the 373th batch\n",
      "Writing the 374th batch\n",
      "Writing the 375th batch\n",
      "Writing the 376th batch\n",
      "Writing the 377th batch\n",
      "Writing the 378th batch\n",
      "Writing the 379th batch\n",
      "Writing the 380th batch\n",
      "Writing the 381th batch\n",
      "Writing the 382th batch\n",
      "Writing the 383th batch\n",
      "Writing the 384th batch\n",
      "Writing the 385th batch\n",
      "Writing the 386th batch\n",
      "Writing the 387th batch\n",
      "Writing the 388th batch\n",
      "Writing the 389th batch\n",
      "Writing the 390th batch\n",
      "Writing the 391th batch\n",
      "Writing the 392th batch\n",
      "Writing the 393th batch\n",
      "Writing the 394th batch\n",
      "Writing the 395th batch\n",
      "Writing the 396th batch\n",
      "Writing the 397th batch\n",
      "Writing the 398th batch\n",
      "Writing the 399th batch\n",
      "Writing the 400th batch\n",
      "Writing the 401th batch\n",
      "Writing the 402th batch\n",
      "Writing the 403th batch\n",
      "Writing the 404th batch\n",
      "Writing the 405th batch\n",
      "Writing the 406th batch\n",
      "Writing the 407th batch\n",
      "Writing the 408th batch\n",
      "Writing the 409th batch\n",
      "Writing the 410th batch\n",
      "Writing the 411th batch\n",
      "Writing the 412th batch\n",
      "Writing the 413th batch\n",
      "Writing the 414th batch\n",
      "Writing the 415th batch\n",
      "Writing the 416th batch\n",
      "Writing the 417th batch\n",
      "Writing the 418th batch\n",
      "Writing the 419th batch\n",
      "Writing the 420th batch\n",
      "Writing the 421th batch\n",
      "Writing the 422th batch\n",
      "Writing the 423th batch\n",
      "Writing the 424th batch\n",
      "Writing the 425th batch\n",
      "Writing the 426th batch\n",
      "Writing the 427th batch\n",
      "Writing the 428th batch\n",
      "Writing the 429th batch\n",
      "Writing the 430th batch\n",
      "Writing the 431th batch\n",
      "Writing the 432th batch\n",
      "Writing the 433th batch\n",
      "Writing the 434th batch\n",
      "Writing the 435th batch\n",
      "Writing the 436th batch\n",
      "Writing the 437th batch\n",
      "Writing the 438th batch\n",
      "Writing the 439th batch\n",
      "Writing the 440th batch\n",
      "Writing the 441th batch\n",
      "Writing the 442th batch\n",
      "Writing the 443th batch\n",
      "Writing the 444th batch\n",
      "Writing the 445th batch\n",
      "Writing the 446th batch\n",
      "Writing the 447th batch\n",
      "Writing the 448th batch\n",
      "Writing the 449th batch\n",
      "Writing the 450th batch\n",
      "Writing the 451th batch\n",
      "Writing the 452th batch\n",
      "Writing the 453th batch\n",
      "Writing the 454th batch\n",
      "Writing the 455th batch\n",
      "Writing the 456th batch\n",
      "Writing the 457th batch\n",
      "Writing the 458th batch\n",
      "Writing the 459th batch\n",
      "Writing the 460th batch\n",
      "Writing the 461th batch\n",
      "Writing the 462th batch\n",
      "Writing the 463th batch\n",
      "Writing the 464th batch\n",
      "Writing the 465th batch\n",
      "Writing the 466th batch\n",
      "Writing the 467th batch\n",
      "Writing the 468th batch\n",
      "Writing the 469th batch\n",
      "Writing the 470th batch\n",
      "Writing the 471th batch\n",
      "Writing the 472th batch\n",
      "Writing the 473th batch\n",
      "Writing the 474th batch\n",
      "Writing the 475th batch\n",
      "Writing the 476th batch\n",
      "Writing the 477th batch\n",
      "Writing the 478th batch\n",
      "Writing the 479th batch\n",
      "Writing the 480th batch\n",
      "Writing the 481th batch\n",
      "Writing the 482th batch\n",
      "Writing the 483th batch\n",
      "Writing the 484th batch\n",
      "Writing the 485th batch\n",
      "Writing the 486th batch\n",
      "Writing the 487th batch\n",
      "Writing the 488th batch\n",
      "Writing the 489th batch\n",
      "Writing the 490th batch\n",
      "Writing the 491th batch\n",
      "Writing the 492th batch\n",
      "Writing the 493th batch\n",
      "Writing the 494th batch\n",
      "Writing the 495th batch\n",
      "Writing the 496th batch\n",
      "Writing the 497th batch\n",
      "Writing the 498th batch\n",
      "Writing the 499th batch\n",
      "Writing the 500th batch\n",
      "Writing the 501th batch\n",
      "Writing the 502th batch\n",
      "Writing the 503th batch\n",
      "Writing the 504th batch\n",
      "Writing the 505th batch\n",
      "Writing the 506th batch\n",
      "Writing the 507th batch\n",
      "Writing the 508th batch\n",
      "Writing the 509th batch\n",
      "Writing the 510th batch\n",
      "Writing the 511th batch\n",
      "Writing the 512th batch\n",
      "Writing the 513th batch\n",
      "Writing the 514th batch\n",
      "Writing the 515th batch\n",
      "Writing the 516th batch\n",
      "Writing the 517th batch\n",
      "Writing the 518th batch\n",
      "Writing the 519th batch\n",
      "Writing the 520th batch\n",
      "Writing the 521th batch\n",
      "Writing the 522th batch\n",
      "Writing the 523th batch\n",
      "Writing the 524th batch\n",
      "Writing the 525th batch\n",
      "Writing the 526th batch\n",
      "Writing the 527th batch\n",
      "Writing the 528th batch\n",
      "Writing the 529th batch\n",
      "Writing the 530th batch\n",
      "Writing the 531th batch\n",
      "Writing the 532th batch\n",
      "Writing the 533th batch\n",
      "Writing the 534th batch\n",
      "Writing the 535th batch\n",
      "Writing the 536th batch\n",
      "Writing the 537th batch\n",
      "Writing the 538th batch\n",
      "Writing the 539th batch\n",
      "Writing the 540th batch\n",
      "Writing the 541th batch\n",
      "Writing the 542th batch\n",
      "Writing the 543th batch\n",
      "Writing the 544th batch\n",
      "Writing the 545th batch\n",
      "Writing the 546th batch\n",
      "Writing the 547th batch\n",
      "Writing the 548th batch\n",
      "Writing the 549th batch\n",
      "Writing the 550th batch\n",
      "Writing the 551th batch\n",
      "Writing the 552th batch\n",
      "Writing the 553th batch\n",
      "Writing the 554th batch\n",
      "Writing the 555th batch\n",
      "Writing the 556th batch\n",
      "Writing the 557th batch\n",
      "Writing the 558th batch\n",
      "Writing the 559th batch\n",
      "Writing the 560th batch\n",
      "Writing the 561th batch\n",
      "Writing the 562th batch\n",
      "Writing the 563th batch\n",
      "Writing the 564th batch\n",
      "Writing the 565th batch\n",
      "Writing the 566th batch\n",
      "Writing the 567th batch\n",
      "Writing the 568th batch\n",
      "Writing the 569th batch\n",
      "Writing the 570th batch\n",
      "Writing the 571th batch\n",
      "Writing the 572th batch\n",
      "Writing the 573th batch\n",
      "Writing the 574th batch\n",
      "Writing the 575th batch\n",
      "Writing the 576th batch\n",
      "Writing the 577th batch\n",
      "Writing the 578th batch\n",
      "Writing the 579th batch\n",
      "Writing the 580th batch\n",
      "Writing the 581th batch\n",
      "Writing the 582th batch\n",
      "Writing the 583th batch\n",
      "Writing the 584th batch\n",
      "Writing the 585th batch\n",
      "Writing the 586th batch\n",
      "Writing the 587th batch\n",
      "Writing the 588th batch\n",
      "Writing the 589th batch\n",
      "Writing the 590th batch\n",
      "Writing the 591th batch\n",
      "Writing the 592th batch\n",
      "Writing the 593th batch\n",
      "Writing the 594th batch\n",
      "Writing the 595th batch\n",
      "Writing the 596th batch\n",
      "Writing the 597th batch\n",
      "Writing the 598th batch\n",
      "Writing the 599th batch\n",
      "Writing the 600th batch\n",
      "Writing the 601th batch\n",
      "Writing the 602th batch\n",
      "Writing the 603th batch\n",
      "Writing the 604th batch\n",
      "Writing the 605th batch\n",
      "Writing the 606th batch\n",
      "Writing the 607th batch\n",
      "Writing the 608th batch\n",
      "Writing the 609th batch\n",
      "Writing the 610th batch\n",
      "Writing the 611th batch\n",
      "Writing the 612th batch\n",
      "Writing the 613th batch\n",
      "Writing the 614th batch\n",
      "Writing the 615th batch\n",
      "Writing the 616th batch\n",
      "Writing the 617th batch\n",
      "Writing the 618th batch\n",
      "Writing the 619th batch\n",
      "Writing the 620th batch\n",
      "Writing the 621th batch\n",
      "Writing the 622th batch\n",
      "Writing the 623th batch\n",
      "Writing the 624th batch\n",
      "Writing the 625th batch\n",
      "Writing the 626th batch\n",
      "Writing the 627th batch\n",
      "Writing the 628th batch\n",
      "Writing the 629th batch\n",
      "Writing the 630th batch\n",
      "Writing the 631th batch\n",
      "Writing the 632th batch\n",
      "Writing the 633th batch\n",
      "Writing the 634th batch\n",
      "Writing the 635th batch\n",
      "Writing the 636th batch\n",
      "Writing the 637th batch\n",
      "Writing the 638th batch\n",
      "Writing the 639th batch\n",
      "Writing the 640th batch\n",
      "Writing the 641th batch\n",
      "Writing the 642th batch\n",
      "Writing the 643th batch\n",
      "Writing the 644th batch\n",
      "Writing the 645th batch\n",
      "Writing the 646th batch\n",
      "Writing the 647th batch\n",
      "Writing the 648th batch\n",
      "Writing the 649th batch\n",
      "Writing the 650th batch\n",
      "Writing the 651th batch\n",
      "Writing the 652th batch\n",
      "Writing the 653th batch\n",
      "Writing the 654th batch\n",
      "Writing the 655th batch\n",
      "Writing the 656th batch\n",
      "Writing the 657th batch\n",
      "Writing the 658th batch\n",
      "Writing the 659th batch\n",
      "Writing the 660th batch\n",
      "Writing the 661th batch\n",
      "Writing the 662th batch\n",
      "Writing the 663th batch\n",
      "Writing the 664th batch\n",
      "Writing the 665th batch\n",
      "Writing the 666th batch\n",
      "Writing the 667th batch\n",
      "Writing the 668th batch\n",
      "Writing the 669th batch\n",
      "Writing the 670th batch\n",
      "Writing the 671th batch\n",
      "Writing the 672th batch\n",
      "Writing the 673th batch\n",
      "Writing the 674th batch\n",
      "Writing the 675th batch\n",
      "Writing the 676th batch\n",
      "Writing the 677th batch\n",
      "Writing the 678th batch\n",
      "Writing the 679th batch\n",
      "Writing the 680th batch\n",
      "Writing the 681th batch\n",
      "Writing the 682th batch\n",
      "Writing the 683th batch\n",
      "Writing the 684th batch\n",
      "Writing the 685th batch\n",
      "Writing the 686th batch\n",
      "Writing the 687th batch\n",
      "Writing the 688th batch\n",
      "Writing the 689th batch\n",
      "Writing the 690th batch\n",
      "Writing the 691th batch\n",
      "Writing the 692th batch\n",
      "Writing the 693th batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the 694th batch\n",
      "Writing the 695th batch\n",
      "Writing the 696th batch\n",
      "Writing the 697th batch\n",
      "Writing the 698th batch\n",
      "Writing the 699th batch\n",
      "Writing the 700th batch\n",
      "Writing the 701th batch\n",
      "Writing the 702th batch\n",
      "Writing the 703th batch\n",
      "Writing the 704th batch\n",
      "Writing the 705th batch\n",
      "Writing the 706th batch\n",
      "Writing the 707th batch\n",
      "Writing the 708th batch\n",
      "Writing the 709th batch\n",
      "Writing the 710th batch\n",
      "Writing the 711th batch\n",
      "Writing the 712th batch\n",
      "Writing the 713th batch\n",
      "Writing the 714th batch\n",
      "Writing the 715th batch\n",
      "Writing the 716th batch\n",
      "Writing the 717th batch\n",
      "Writing the 718th batch\n",
      "Writing the 719th batch\n",
      "Writing the 720th batch\n",
      "Writing the 721th batch\n",
      "Writing the 722th batch\n",
      "Writing the 723th batch\n",
      "Writing the 724th batch\n",
      "Writing the 725th batch\n",
      "Writing the 726th batch\n",
      "Writing the 727th batch\n",
      "Writing the 728th batch\n",
      "Writing the 729th batch\n",
      "Writing the 730th batch\n",
      "Writing the 731th batch\n",
      "Writing the 732th batch\n",
      "Writing the 733th batch\n",
      "Writing the 734th batch\n",
      "Writing the 735th batch\n",
      "Writing the 736th batch\n",
      "Writing the 737th batch\n",
      "Writing the 738th batch\n",
      "Writing the 739th batch\n",
      "Writing the 740th batch\n",
      "Writing the 741th batch\n",
      "Writing the 742th batch\n",
      "Writing the 743th batch\n",
      "Writing the 744th batch\n",
      "Writing the 745th batch\n",
      "Writing the 746th batch\n",
      "Writing the 747th batch\n",
      "Writing the 748th batch\n",
      "Writing the 749th batch\n",
      "Writing the 750th batch\n",
      "Writing the 751th batch\n",
      "Writing the 752th batch\n",
      "Writing the 753th batch\n",
      "Writing the 754th batch\n",
      "Writing the 755th batch\n",
      "Writing the 756th batch\n",
      "Writing the 757th batch\n",
      "Writing the 758th batch\n",
      "Writing the 759th batch\n",
      "Writing the 760th batch\n",
      "Writing the 761th batch\n",
      "Writing the 762th batch\n",
      "Writing the 763th batch\n",
      "Writing the 764th batch\n",
      "Writing the 765th batch\n",
      "Writing the 766th batch\n",
      "Writing the 767th batch\n",
      "Writing the 768th batch\n",
      "Writing the 769th batch\n",
      "Writing the 770th batch\n",
      "Writing the 771th batch\n",
      "Writing the 772th batch\n",
      "Writing the 773th batch\n",
      "Writing the 774th batch\n",
      "Writing the 775th batch\n",
      "Writing the 776th batch\n",
      "Writing the 777th batch\n",
      "Writing the 778th batch\n",
      "Writing the 779th batch\n",
      "Writing the 780th batch\n",
      "Writing the 781th batch\n",
      "Writing the 782th batch\n",
      "Writing the 783th batch\n",
      "Writing the 784th batch\n",
      "Writing the 785th batch\n",
      "Writing the 786th batch\n",
      "Writing the 787th batch\n",
      "Writing the 788th batch\n",
      "Writing the 789th batch\n",
      "Writing the 790th batch\n",
      "Writing the 791th batch\n",
      "Writing the 792th batch\n",
      "Writing the 793th batch\n",
      "Writing the 794th batch\n",
      "Writing the 795th batch\n",
      "Writing the 796th batch\n",
      "Writing the 797th batch\n",
      "Writing the 798th batch\n",
      "Writing the 799th batch\n",
      "Writing the 800th batch\n",
      "Writing the 801th batch\n",
      "Writing the 802th batch\n",
      "Writing the 803th batch\n",
      "Writing the 804th batch\n",
      "Writing the 805th batch\n",
      "Writing the 806th batch\n",
      "Writing the 807th batch\n",
      "Writing the 808th batch\n",
      "Writing the 809th batch\n",
      "Writing the 810th batch\n",
      "Writing the 811th batch\n",
      "Writing the 812th batch\n",
      "Writing the 813th batch\n",
      "Writing the 814th batch\n",
      "Writing the 815th batch\n",
      "Writing the 816th batch\n",
      "Writing the 817th batch\n",
      "Writing the 818th batch\n",
      "Writing the 819th batch\n",
      "Writing the 820th batch\n",
      "Writing the 821th batch\n",
      "Writing the 822th batch\n",
      "Writing the 823th batch\n",
      "Writing the 824th batch\n",
      "Writing the 825th batch\n",
      "Writing the 826th batch\n",
      "Writing the 827th batch\n",
      "Writing the 828th batch\n",
      "Writing the 829th batch\n",
      "Writing the 830th batch\n",
      "Writing the 831th batch\n",
      "Writing the 832th batch\n",
      "Writing the 833th batch\n",
      "Writing the 834th batch\n",
      "Writing the 835th batch\n",
      "Writing the 836th batch\n",
      "Writing the 837th batch\n",
      "Writing the 838th batch\n",
      "Writing the 839th batch\n",
      "Writing the 840th batch\n",
      "Writing the 841th batch\n",
      "Writing the 842th batch\n",
      "Writing the 843th batch\n",
      "Writing the 844th batch\n",
      "Writing the 845th batch\n",
      "Writing the 846th batch\n",
      "Writing the 847th batch\n",
      "Writing the 848th batch\n",
      "Writing the 849th batch\n",
      "Writing the 850th batch\n",
      "Writing the 851th batch\n",
      "Writing the 852th batch\n",
      "Writing the 853th batch\n",
      "Writing the 854th batch\n",
      "Writing the 855th batch\n",
      "Writing the 856th batch\n",
      "Writing the 857th batch\n",
      "Writing the 858th batch\n",
      "Writing the 859th batch\n",
      "Writing the 860th batch\n",
      "Writing the 861th batch\n",
      "Writing the 862th batch\n",
      "Writing the 863th batch\n",
      "Writing the 864th batch\n",
      "Writing the 865th batch\n",
      "Writing the 866th batch\n",
      "Writing the 867th batch\n",
      "Writing the 868th batch\n",
      "Writing the 869th batch\n",
      "Writing the 870th batch\n",
      "Writing the 871th batch\n",
      "Writing the 872th batch\n",
      "Writing the 873th batch\n",
      "Writing the 874th batch\n",
      "Writing the 875th batch\n",
      "Writing the 876th batch\n",
      "Writing the 877th batch\n",
      "Writing the 878th batch\n",
      "Writing the 879th batch\n",
      "Writing the 880th batch\n",
      "Writing the 881th batch\n",
      "Writing the 882th batch\n",
      "Writing the 883th batch\n",
      "Writing the 884th batch\n",
      "Writing the 885th batch\n",
      "Writing the 886th batch\n",
      "Writing the 887th batch\n",
      "Writing the 888th batch\n",
      "Writing the 889th batch\n",
      "Writing the 890th batch\n",
      "Writing the 891th batch\n",
      "Writing the 892th batch\n",
      "Writing the 893th batch\n",
      "Writing the 894th batch\n",
      "Writing the 895th batch\n",
      "Writing the 896th batch\n",
      "Writing the 897th batch\n",
      "Writing the 898th batch\n",
      "Writing the 899th batch\n",
      "Writing the 900th batch\n",
      "Writing the 901th batch\n",
      "Writing the 902th batch\n",
      "Writing the 903th batch\n",
      "Writing the 904th batch\n",
      "Writing the 905th batch\n",
      "Writing the 906th batch\n",
      "Writing the 907th batch\n",
      "Writing the 908th batch\n",
      "Writing the 909th batch\n",
      "Writing the 910th batch\n",
      "Writing the 911th batch\n",
      "Writing the 912th batch\n",
      "Writing the 913th batch\n",
      "Writing the 914th batch\n",
      "Writing the 915th batch\n",
      "Writing the 916th batch\n",
      "Writing the 917th batch\n",
      "Writing the 918th batch\n",
      "Writing the 919th batch\n",
      "Writing the 920th batch\n",
      "Writing the 921th batch\n",
      "Writing the 922th batch\n",
      "Writing the 923th batch\n",
      "Writing the 924th batch\n",
      "Writing the 925th batch\n",
      "Writing the 926th batch\n",
      "Writing the 927th batch\n",
      "Writing the 928th batch\n",
      "Writing the 929th batch\n",
      "Writing the 930th batch\n",
      "Writing the 931th batch\n",
      "Writing the 932th batch\n",
      "Writing the 933th batch\n",
      "Writing the 934th batch\n",
      "Writing the 935th batch\n",
      "Writing the 936th batch\n",
      "Writing the 937th batch\n",
      "Writing the 938th batch\n",
      "Writing the 939th batch\n",
      "Writing the 940th batch\n",
      "Writing the 941th batch\n",
      "Writing the 942th batch\n",
      "Writing the 943th batch\n",
      "Writing the 944th batch\n",
      "Writing the 945th batch\n",
      "Writing the 946th batch\n",
      "Writing the 947th batch\n",
      "Writing the 948th batch\n",
      "Writing the 949th batch\n",
      "Writing the 950th batch\n",
      "Writing the 951th batch\n",
      "Writing the 952th batch\n",
      "Writing the 953th batch\n",
      "Writing the 954th batch\n",
      "Writing the 955th batch\n",
      "Writing the 956th batch\n",
      "Writing the 957th batch\n",
      "Writing the 958th batch\n",
      "Writing the 959th batch\n",
      "Writing the 960th batch\n",
      "Writing the 961th batch\n",
      "Writing the 962th batch\n",
      "Writing the 963th batch\n",
      "Writing the 964th batch\n",
      "Writing the 965th batch\n",
      "Writing the 966th batch\n",
      "Writing the 967th batch\n",
      "Writing the 968th batch\n",
      "Writing the 969th batch\n",
      "Writing the 970th batch\n",
      "Writing the 971th batch\n",
      "Writing the 972th batch\n",
      "Writing the 973th batch\n",
      "Writing the 974th batch\n",
      "Writing the 975th batch\n",
      "Writing the 976th batch\n",
      "Writing the 977th batch\n",
      "Writing the 978th batch\n",
      "Writing the 979th batch\n",
      "Writing the 980th batch\n",
      "Writing the 981th batch\n",
      "Writing the 982th batch\n",
      "Writing the 983th batch\n",
      "Writing the 984th batch\n",
      "Writing the 985th batch\n",
      "Writing the 986th batch\n",
      "Writing the 987th batch\n",
      "Writing the 988th batch\n",
      "Writing the 989th batch\n",
      "Writing the 990th batch\n",
      "Writing the 991th batch\n",
      "Writing the 992th batch\n",
      "Writing the 993th batch\n",
      "Writing the 994th batch\n",
      "Writing the 995th batch\n",
      "Writing the 996th batch\n",
      "Writing the 997th batch\n",
      "Writing the 998th batch\n",
      "Writing the 999th batch\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "configs_written = 0\n",
    "size = batch_size\n",
    "write_header = True\n",
    "config_output_path = \"../data/output/\"\n",
    "\n",
    "file_name = join(config_output_path, \"right_\" + str(num_joints) + '_' + str(generated_sample_size) + '.csv')\n",
    "print(\"Writing samples to: %s\" % (file_name))\n",
    "\n",
    "for i in range(math.ceil(generated_sample_size / batch_size)):\n",
    "    print(\"Writing the %dth batch\" % i)\n",
    "    if generated_sample_size - configs_written < batch_size:\n",
    "        size = generated_sample_size - configs_written\n",
    "    samples = generate_samples(size, d_output, device, model)\n",
    "    if i > 0:\n",
    "        write_header = False\n",
    "    write_samples_to_csv(file_name, samples, joint_limits, headers, \n",
    "                         HEADERS, write_header=write_header, print_frame=False)\n",
    "    configs_written += size\n",
    "\n",
    "with open(os.path.join(output_directory, 'model.pth'), 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
